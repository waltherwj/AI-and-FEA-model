{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING\n",
    "This notebook has as an objective to use a folder containing the formated .npy files to train a deep learning model that can be used in lieu of an FEA model to any accuracy above random, for a proof of concept that it is possible to do so, such that further research can be done afterward to optimize architecture, hyperparameters and data being fed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pathlib\n",
    "import PREPROCESSING_splitting as split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, create a class to load the files that are to be fed to the neural network, \n",
    "## for both the inputs and the outputs. To avoid confusion I'll refer to the inputs to the \n",
    "## FEA model as 'bound_conds' (boundary conditions), and the outputs as 'targets', while what \n",
    "## is fed into the neural network will be called an \"input\", and the output of the neural network \"prediction\"\n",
    "## this function should probably be transformed into a dataloader later for a larger dataset, \n",
    "## but for now we'll keep it like this\n",
    "\n",
    "def get_dataset(dataset_path, glob_parameter = '*.npy'):\n",
    "    # concatenates all samples into a list of boundary conditions and a list of targets\n",
    "    \n",
    "    # set paths\n",
    "    bound_cond_path = pathlib.Path(dataset_path, 'input')\n",
    "    targets_path = pathlib.Path(dataset_path, 'output')\n",
    "\n",
    "    test = pathlib.Path('D:/')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # check if folder path is correct\n",
    "    if bound_cond_path.is_dir() and targets_path.is_dir():\n",
    "        print('path contains \\'input\\' and \\'output\\'')\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception (f'Argument dataset_path: {dataset_path} should contain folders ..\\input and ..\\output. Please check path')\n",
    "    \n",
    "    #create iterators for files\n",
    "    bound_cond_iterator = bound_cond_path.glob(glob_parameter)\n",
    "    targets_iterator = targets_path.glob(glob_parameter)\n",
    "    \n",
    "    #zip them to ensure that they are going through the same samples \n",
    "    samples_iterator = zip(bound_cond_iterator, targets_iterator)\n",
    "    \n",
    "    boundary_conditions = np.array([])\n",
    "    targets = np.array([])\n",
    "    \n",
    "    for boundary_condition_files, targets_files in samples_iterator:\n",
    "        if split.get_number(boundary_condition_files.name) == split.get_number(targets_files.name):\n",
    "            \n",
    "            boundary_conditions_temp = np.load(boundary_condition_files)\n",
    "            targets_temp = np.load(targets_files)\n",
    "            \n",
    "            #start array if it hasn't been started yet\n",
    "            if boundary_conditions.size == 0 and targets.size == 0:\n",
    "                boundary_conditions = boundary_conditions_temp\n",
    "                targets = targets_temp\n",
    "            else:\n",
    "                boundary_conditions = np.concatenate((boundary_conditions, boundary_conditions_temp), axis = 0)\n",
    "                targets = np.concatenate((targets, targets_temp), axis = 0)\n",
    "        else:\n",
    "            raise Exception('the samples in the iterator are not synced')\n",
    "    \n",
    "    return torch.from_numpy(boundary_conditions).float(), torch.from_numpy(targets).float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path contains 'input' and 'output'\n",
      "torch.Size([102, 7, 32, 32])\n",
      "torch.Size([102, 4, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "folders_path =  pathlib.Path('D:/Ansys Simulations/Project/2D/data/proof_of_concept/scaled/arrays')\n",
    "dataset = get_dataset(dataset_path = folders_path)\n",
    "print(dataset[0].shape)\n",
    "print(dataset[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoD_Dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, npy_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            npy_file (string): Path to the npy files.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.boundary_conditions = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0899,  0.1401,  0.1367,  ...,  0.1344,  0.1332,  0.1642],\n",
       "          [ 0.0275,  0.0331,  0.0501,  ...,  0.0514,  0.0402,  0.0876],\n",
       "          [ 0.0151,  0.0327,  0.0461,  ...,  0.0434,  0.0454,  0.0939],\n",
       "          ...,\n",
       "          [ 0.0117,  0.0397,  0.0714,  ...,  0.0555,  0.0526,  0.0994],\n",
       "          [ 0.0217,  0.0424,  0.0708,  ...,  0.0494,  0.0496,  0.0974],\n",
       "          [ 0.0364,  0.0131,  0.0366,  ...,  0.0214,  0.0174,  0.0634]],\n",
       "\n",
       "         [[ 0.1623,  0.1757,  0.1672,  ...,  0.1696,  0.1659,  0.1744],\n",
       "          [ 0.1989,  0.1802,  0.1755,  ...,  0.1765,  0.1681,  0.1373],\n",
       "          [ 0.1989,  0.1784,  0.1785,  ...,  0.1796,  0.1698,  0.1390],\n",
       "          ...,\n",
       "          [ 0.1771,  0.1524,  0.1432,  ...,  0.1626,  0.1582,  0.1340],\n",
       "          [ 0.1951,  0.1699,  0.1705,  ...,  0.1940,  0.1766,  0.1503],\n",
       "          [ 0.2255,  0.2246,  0.2194,  ...,  0.2200,  0.2151,  0.1395]],\n",
       "\n",
       "         [[-0.2076, -0.2259, -0.2587,  ..., -0.2501, -0.2422, -0.2105],\n",
       "          [-0.1878, -0.2640, -0.3071,  ..., -0.2944, -0.2938, -0.2552],\n",
       "          [-0.1954, -0.2738, -0.3077,  ..., -0.2901, -0.2849, -0.2471],\n",
       "          ...,\n",
       "          [-0.2006, -0.2768, -0.3069,  ..., -0.2910, -0.2863, -0.2477],\n",
       "          [-0.2199, -0.3100, -0.3328,  ..., -0.3136, -0.3061, -0.2612],\n",
       "          [-0.2263, -0.2958, -0.3064,  ..., -0.3029, -0.3036, -0.2706]]]],\n",
       "       grad_fn=<HardtanhBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## define the neural network's general shape\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        ## convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels = 7, out_channels = 14, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 14, out_channels = 16, kernel_size = 3, padding = 1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels = 16, out_channels = 14, kernel_size = 3, padding = 1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels = 14, out_channels = 10, kernel_size = 3, padding = 1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels = 10, out_channels = 7, kernel_size = 3, padding = 1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(in_channels = 7, out_channels = 3, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        ## activation\n",
    "        self.hardtanh = nn.Hardtanh()\n",
    "        \n",
    "        \n",
    "        ##possible for later: MultiheadAttention\n",
    "        \n",
    "    def forward(self, boundary_conditions):\n",
    "        #print(boundary_conditions.shape)\n",
    "        x = self.conv1(boundary_conditions)\n",
    "        x = self.hardtanh(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.hardtanh(x)\n",
    "        #print(x.shape)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.hardtanh(x)\n",
    "       #print(x.shape)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.hardtanh(x)\n",
    "        #print(x.shape)\n",
    "        x = self.deconv3(x)\n",
    "        x = self.hardtanh(x)\n",
    "       # print(x.shape)\n",
    "        x = self.deconv4(x)\n",
    "        x = self.hardtanh(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "net = ConvNet().float()\n",
    "\n",
    "## test to see if getting the correct size\n",
    "net.forward(dataset[0][0:1,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define criterion\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params = net.parameters(), lr=0.0001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0.03659322112798691 0.043989747017622\n",
      "2 1 0.0020094902720302343 0.0022937262430787085\n",
      "3 1 0.0015545941423624754 0.0018099371693097055\n",
      "4 1 0.001284114783629775 0.0015103648998774587\n",
      "5 1 0.001086575910449028 0.0012802096316590905\n",
      "6 1 0.0009334570495411754 0.0010969194176141174\n",
      "7 1 0.0008097579120658338 0.0009486907161772251\n",
      "8 1 0.0007059931522235274 0.0008243066491559148\n",
      "9 1 0.000616582459770143 0.000717484694905579\n",
      "10 1 0.0005400271038524806 0.000627140054712072\n",
      "11 1 0.00047517954953946173 0.0005520317761693149\n",
      "12 1 0.0004204793367534876 0.0004899854509858414\n",
      "13 1 0.0003749725001398474 0.0004391036054585129\n",
      "14 1 0.0003376926470082253 0.00039748791023157536\n",
      "15 1 0.00030743962270207703 0.00036348945286590606\n",
      "16 1 0.00028301571728661656 0.0003358305606525391\n",
      "17 1 0.00026335837901569903 0.0003134726357529871\n",
      "18 1 0.0002475546789355576 0.00029551324550993743\n",
      "19 1 0.00023482968390453607 0.0002811559199471958\n",
      "20 1 0.0002245364448754117 0.00026970973558491094\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "## define a general training loop\n",
    "losses = np.array([])\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    boundary_conditions, targets = dataset[0][:90,:,:,:], dataset[1][:90,:,:,:]\n",
    "    #print(type(boundary_conditions))\n",
    "    for i, bc in enumerate(boundary_conditions):\n",
    "        tgt = targets[i, 1:4, :, :].unsqueeze(0)\n",
    "        bc = bc.unsqueeze(0)\n",
    "        #print(i, bc.shape, tgt.shape)\n",
    "\n",
    "        # get the inputs; data is a list of [boundary_conditions, targets]\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        prediction = net(bc)\n",
    "        loss = criterion(prediction, tgt)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:    # print every 10 mini-batches\n",
    "            running_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                boundary_conditions_val, targets_val = dataset[0][90:,:,:,:], dataset[1][90:,:,:,:]\n",
    "                for ii, bc_val in enumerate(boundary_conditions_val):\n",
    "                    tgt_val = targets_val[i, 1:4, :, :].unsqueeze(0)\n",
    "                    bc_val = bc_val.unsqueeze(0)\n",
    "                    prediction_val = net(bc_val)\n",
    "                    val_loss = criterion(prediction_val, tgt_val)\n",
    "                    running_val_loss+=val_loss.item()\n",
    "                \n",
    "            print(f'{epoch + 1} {i + 1} {running_loss} {running_val_loss/10}')\n",
    "            np.append(losses, [running_loss, running_val_loss])\n",
    "            running_loss = 0.0\n",
    "            \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "## attempt to plot losses over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
