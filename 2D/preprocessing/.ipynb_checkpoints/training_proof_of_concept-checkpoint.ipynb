{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING\n",
    "This notebook has as an objective to use a folder containing the formated .npy files to train a deep learning model that can be used in lieu of an FEA model to any accuracy above random, for a proof of concept that it is possible to do so, such that further research can be done afterward to optimize architecture, hyperparameters and data being fed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pathlib\n",
    "import PREPROCESSING_splitting as split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, create a class to load the files that are to be fed to the neural network, \n",
    "## for both the inputs and the outputs. To avoid confusion I'll refer to the inputs to the \n",
    "## FEA model as 'bound_conds' (boundary conditions), and the outputs as 'targets', while what \n",
    "## is fed into the neural network will be called an \"input\", and the output of the neural network \"prediction\"\n",
    "## this function should probably be transformed into a dataloader later for a larger dataset, \n",
    "## but for now we'll keep it like this\n",
    "\n",
    "def get_dataset(dataset_path, glob_parameter = '*.npy'):\n",
    "    # concatenates all samples into a list of boundary conditions and a list of targets\n",
    "    \n",
    "    # set paths\n",
    "    bound_cond_path = pathlib.Path(dataset_path, 'input')\n",
    "    targets_path = pathlib.Path(dataset_path, 'output')\n",
    "\n",
    "    test = pathlib.Path('D:/')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # check if folder path is correct\n",
    "    if bound_cond_path.is_dir() and targets_path.is_dir():\n",
    "        print('path contains \\'input\\' and \\'output\\'')\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception (f'Argument dataset_path: {dataset_path} should contain folders ..\\input and ..\\output. Please check path')\n",
    "    \n",
    "    #create iterators for files\n",
    "    bound_cond_iterator = bound_cond_path.glob(glob_parameter)\n",
    "    targets_iterator = targets_path.glob(glob_parameter)\n",
    "    \n",
    "    #zip them to ensure that they are going through the same samples \n",
    "    samples_iterator = zip(bound_cond_iterator, targets_iterator)\n",
    "    \n",
    "    boundary_conditions = np.array([])\n",
    "    targets = np.array([])\n",
    "    \n",
    "    for boundary_condition_files, targets_files in samples_iterator:\n",
    "        if split.get_number(boundary_condition_files.name) == split.get_number(targets_files.name):\n",
    "            \n",
    "            boundary_conditions_temp = np.load(boundary_condition_files)\n",
    "            targets_temp = np.load(targets_files)\n",
    "            \n",
    "            #start array if it hasn't been started yet\n",
    "            if boundary_conditions.size == 0 and targets.size == 0:\n",
    "                boundary_conditions = boundary_conditions_temp\n",
    "                targets = targets_temp\n",
    "            else:\n",
    "                boundary_conditions = np.concatenate((boundary_conditions, boundary_conditions_temp), axis = 0)\n",
    "                targets = np.concatenate((targets, targets_temp), axis = 0)\n",
    "        else:\n",
    "            raise Exception('the samples in the iterator are not synced')\n",
    "    \n",
    "    return torch.from_numpy(boundary_conditions).float(), torch.from_numpy(targets).float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path contains 'input' and 'output'\n",
      "torch.Size([102, 7, 32, 32])\n",
      "torch.Size([102, 4, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "folders_path =  pathlib.Path('D:/Ansys Simulations/Project/2D/data/proof_of_concept/scaled/arrays')\n",
    "dataset = get_dataset(dataset_path = folders_path)\n",
    "print(dataset[0].shape)\n",
    "print(dataset[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 32, 32])\n",
      "torch.Size([1, 14, 32, 32])\n",
      "torch.Size([1, 16, 32, 32])\n",
      "torch.Size([1, 14, 32, 32])\n",
      "torch.Size([1, 10, 32, 32])\n",
      "torch.Size([1, 7, 32, 32])\n",
      "torch.Size([1, 4, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0563,  0.0505,  0.0692,  ...,  0.0724,  0.0842,  0.0798],\n",
       "          [ 0.0645,  0.0497,  0.0631,  ...,  0.0630,  0.0623,  0.0594],\n",
       "          [ 0.0647,  0.0423,  0.0532,  ...,  0.0513,  0.0518,  0.0399],\n",
       "          ...,\n",
       "          [ 0.0584,  0.0433,  0.0514,  ...,  0.0490,  0.0464,  0.0358],\n",
       "          [ 0.0692,  0.0554,  0.0643,  ...,  0.0624,  0.0570,  0.0475],\n",
       "          [ 0.0636,  0.0590,  0.0611,  ...,  0.0552,  0.0498,  0.0397]],\n",
       "\n",
       "         [[ 0.1094,  0.0712,  0.0834,  ...,  0.0849,  0.0930,  0.0912],\n",
       "          [ 0.1169,  0.0748,  0.0849,  ...,  0.0843,  0.0875,  0.0867],\n",
       "          [ 0.1309,  0.0825,  0.0902,  ...,  0.0876,  0.0821,  0.0744],\n",
       "          ...,\n",
       "          [ 0.1380,  0.0717,  0.0836,  ...,  0.0937,  0.0868,  0.0778],\n",
       "          [ 0.1240,  0.0726,  0.0830,  ...,  0.0977,  0.0904,  0.0858],\n",
       "          [ 0.1104,  0.0538,  0.0558,  ...,  0.0730,  0.0604,  0.0698]],\n",
       "\n",
       "         [[ 0.0116,  0.0229,  0.0184,  ...,  0.0200,  0.0108, -0.0122],\n",
       "          [ 0.0854,  0.0631,  0.0712,  ...,  0.0721,  0.0569,  0.0064],\n",
       "          [ 0.0911,  0.0455,  0.0568,  ...,  0.0582,  0.0412, -0.0141],\n",
       "          ...,\n",
       "          [ 0.0964,  0.0513,  0.0582,  ...,  0.0546,  0.0370, -0.0168],\n",
       "          [ 0.1042,  0.0522,  0.0605,  ...,  0.0498,  0.0355, -0.0180],\n",
       "          [ 0.0780,  0.0463,  0.0549,  ...,  0.0370,  0.0242, -0.0087]],\n",
       "\n",
       "         [[ 0.1620,  0.1257,  0.1289,  ...,  0.1302,  0.1148,  0.0837],\n",
       "          [ 0.2084,  0.1748,  0.1874,  ...,  0.1859,  0.1684,  0.1148],\n",
       "          [ 0.2166,  0.1775,  0.1937,  ...,  0.1927,  0.1777,  0.1135],\n",
       "          ...,\n",
       "          [ 0.2061,  0.1611,  0.1814,  ...,  0.1881,  0.1748,  0.1165],\n",
       "          [ 0.2010,  0.1665,  0.1834,  ...,  0.1857,  0.1756,  0.1109],\n",
       "          [ 0.1558,  0.1149,  0.1234,  ...,  0.1197,  0.1225,  0.1256]]]],\n",
       "       grad_fn=<HardtanhBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## define the neural network's general shape\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        ## convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels = 7, out_channels = 14, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 14, out_channels = 16, kernel_size = 3, padding = 1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels = 16, out_channels = 14, kernel_size = 3, padding = 1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels = 14, out_channels = 10, kernel_size = 3, padding = 1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels = 10, out_channels = 7, kernel_size = 3, padding = 1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(in_channels = 7, out_channels = 4, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        ## activation\n",
    "        self.hardtanh = nn.Hardtanh()\n",
    "        \n",
    "        \n",
    "        ##possible for later: MultiheadAttention\n",
    "        \n",
    "    def forward(self, boundary_conditions):\n",
    "        print(boundary_conditions.shape)\n",
    "        x = self.conv1(boundary_conditions)\n",
    "        x = self.hardtanh(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.hardtanh(x)\n",
    "        print(x.shape)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.hardtanh(x)\n",
    "        print(x.shape)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.hardtanh(x)\n",
    "        print(x.shape)\n",
    "        x = self.deconv3(x)\n",
    "        x = self.hardtanh(x)\n",
    "        print(x.shape)\n",
    "        x = self.deconv4(x)\n",
    "        x = self.hardtanh(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "net = ConvNet().float()\n",
    "\n",
    "## test to see if getting the correct size\n",
    "net.forward(dataset[0][0:1,:,:,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
