{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING\n",
    "This notebook has as an objective to use a folder containing the formated .npy files to train a deep learning model that can be used in lieu of an FEA model to any accuracy above random, for a proof of concept that it is possible to do so, such that further research can be done afterward to optimize architecture, hyperparameters and data being fed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting functions imported\n"
     ]
    }
   ],
   "source": [
    "##  imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib\n",
    "import PREPROCESSING_splitting as split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, create a class to load the files that are to be fed to the neural network, \n",
    "## for both the inputs and the outputs. To avoid confusion I'll refer to the inputs to the \n",
    "## FEA model as 'bound_conds' (boundary conditions), and the outputs as 'targets', while what \n",
    "## is fed into the neural network will be called an \"input\", and the output of the neural network \"prediction\"\n",
    "## this function should probably be transformed into a dataloader later for a larger dataset, \n",
    "## but for now we'll keep it like this\n",
    "\n",
    "def get_dataset(dataset_path, glob_parameter = '*.npy'):\n",
    "    # concatenates all samples into a list of boundary conditions and a list of targets\n",
    "    \n",
    "    # set paths\n",
    "    bound_cond_path = pathlib.Path(dataset_path, 'input')\n",
    "    targets_path = pathlib.Path(dataset_path, 'output')\n",
    "\n",
    "    test = pathlib.Path('D:/')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # check if folder path is correct\n",
    "    if bound_cond_path.is_dir() and targets_path.is_dir():\n",
    "        print('path contains \\'input\\' and \\'output\\'')\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception (f'Argument dataset_path: {dataset_path} should contain folders ..\\input and ..\\output. Please check path')\n",
    "    \n",
    "    #create iterators for files\n",
    "    bound_cond_iterator = bound_cond_path.glob(glob_parameter)\n",
    "    targets_iterator = targets_path.glob(glob_parameter)\n",
    "    \n",
    "    #zip them to ensure that they are going through the same samples \n",
    "    samples_iterator = zip(bound_cond_iterator, targets_iterator)\n",
    "    \n",
    "    boundary_conditions = np.array([])\n",
    "    targets = np.array([])\n",
    "    \n",
    "    for boundary_condition_files, targets_files in samples_iterator:\n",
    "        if split.get_number(boundary_condition_files.name) == split.get_number(targets_files.name):\n",
    "            \n",
    "            boundary_conditions_temp = np.load(boundary_condition_files)\n",
    "            targets_temp = np.load(targets_files)\n",
    "            \n",
    "            #start array if it hasn't been started yet\n",
    "            if boundary_conditions.size == 0 and targets.size == 0:\n",
    "                boundary_conditions = boundary_conditions_temp\n",
    "                targets = targets_temp\n",
    "            else:\n",
    "                boundary_conditions = np.concatenate((boundary_conditions, boundary_conditions_temp), axis = 0)\n",
    "                targets = np.concatenate((targets, targets_temp), axis = 0)\n",
    "        else:\n",
    "            raise Exception('the samples in the iterator are not synced')\n",
    "            \n",
    "    return boundary_conditions, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path contains 'input' and 'output'\n"
     ]
    }
   ],
   "source": [
    "folders_path =  pathlib.Path('D:/Ansys Simulations/Project/2D/data/proof_of_concept/scaled/arrays')\n",
    "dataset = get_dataset(dataset_path = folders_path)\n",
    "#dataset.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
