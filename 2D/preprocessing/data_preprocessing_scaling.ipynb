{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Ansys Simulations\\Project\\2D\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCALING NOTEBOOK\n",
    "This notebook is used to develop the functions needed to script the scaling of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PREPROCESSING_splitting'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-47ce4903dc37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPREPROCESSING_splitting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PREPROCESSING_splitting'"
     ]
    }
   ],
   "source": [
    "## imports\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PREPROCESSING_splitting import get_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Ansys Simulations\\Project\\2D\\data\n"
     ]
    }
   ],
   "source": [
    "## Create a function to get the data\n",
    "def get_sample_dfs(samples_folder_path, sample_number):\n",
    "    ## returns the input and output dataframe for the sample specified\n",
    "    \n",
    "    input_folder_path = Path(samples_folder_path, 'input')\n",
    "    output_folder_path = Path(samples_folder_path, 'output')\n",
    "    \n",
    "    glob_string = \"*_\" + str(sample_number) + \".csv\"\n",
    "    \n",
    "    input_sample_glob = input_folder_path.glob(glob_string)\n",
    "    \n",
    "    for i, sample_input_file in enumerate(input_sample_glob):\n",
    "        if i == 0:\n",
    "            sample_input_df = pd.read_csv(sample_input_file, index_col = 0)\n",
    "        else:\n",
    "            raise Exception('error: more than one input sample with label' + str(sample_number))\n",
    "    \n",
    "    output_sample_glob = output_folder_path.glob(glob_string)\n",
    "    \n",
    "    for i, sample_output_file in enumerate(output_sample_glob):\n",
    "        if i == 0:\n",
    "            sample_output_df = pd.read_csv(sample_output_file, index_col = 0)\n",
    "        else:\n",
    "            raise Exception('error: more than one output sample with label' + str(sample_number))\n",
    "    \n",
    "    return sample_input_df, sample_output_df\n",
    "\n",
    "data_folder_path =  Path('D:/Ansys Simulations/Project/2D/data') \n",
    "print(data_folder_path)\n",
    "raw_input_data, raw_output_data = get_sample_dfs(data_folder_path, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_number</th>\n",
       "      <th>x_loc</th>\n",
       "      <th>y_loc</th>\n",
       "      <th>z_loc</th>\n",
       "      <th>x_disp</th>\n",
       "      <th>y_disp</th>\n",
       "      <th>z_disp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.186530</td>\n",
       "      <td>1.13540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000852</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000586</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.178300</td>\n",
       "      <td>1.08530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>-0.000852</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.170060</td>\n",
       "      <td>1.03510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000852</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.161820</td>\n",
       "      <td>0.98496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>-0.000849</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>570</td>\n",
       "      <td>0.169450</td>\n",
       "      <td>0.33947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>-0.000553</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>571</td>\n",
       "      <td>0.140460</td>\n",
       "      <td>0.33622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>-0.000460</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>572</td>\n",
       "      <td>0.104840</td>\n",
       "      <td>0.44160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>573</td>\n",
       "      <td>0.556150</td>\n",
       "      <td>0.75061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.000399</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>574</td>\n",
       "      <td>-0.034216</td>\n",
       "      <td>0.99149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     node_number     x_loc    y_loc  z_loc    x_disp    y_disp  z_disp\n",
       "0              1 -0.186530  1.13540    0.0 -0.000264 -0.000852     0.0\n",
       "1              2  0.000000  0.00000    0.0 -0.000586 -0.000198     0.0\n",
       "2              3 -0.178300  1.08530    0.0 -0.000265 -0.000852     0.0\n",
       "3              4 -0.170060  1.03510    0.0 -0.000264 -0.000852     0.0\n",
       "4              5 -0.161820  0.98496    0.0 -0.000261 -0.000849     0.0\n",
       "..           ...       ...      ...    ...       ...       ...     ...\n",
       "569          570  0.169450  0.33947    0.0  0.000249 -0.000553     0.0\n",
       "570          571  0.140460  0.33622    0.0  0.000280 -0.000460     0.0\n",
       "571          572  0.104840  0.44160    0.0  0.000606 -0.000288     0.0\n",
       "572          573  0.556150  0.75061    0.0  0.000115 -0.000399     0.0\n",
       "573          574 -0.034216  0.99149    0.0 -0.000257 -0.000864     0.0\n",
       "\n",
       "[574 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pi Theorem\n",
    "We can use the pi theorem to undimensionalize the data. Using the pint package to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pint import pi_theorem, formatter, UnitRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disp ** 2 * youngs_modulus / nodal_force\n"
     ]
    }
   ],
   "source": [
    "ureg = UnitRegistry()\n",
    "pi_groups = ureg.pi_theorem({'nodal_force': '[force]',\n",
    "                        'disp': '[length]',\n",
    "                        'youngs_modulus':'[pressure]'})\n",
    "for group in pi_groups:\n",
    "      print(formatter(group.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nodal_force': -1.0, 'disp': 2.0, 'youngs_modulus': 1.0}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we arrive at a problem. With only the input values it is not possible to arrive at representative pi groups. Some alternative approaches could be:\n",
    "* With the idea of having a data-centric approach for the model, what can be done is to scale all the forces by the maximum force in the dataset and all displacements by the maximum displacement in the dataset, and make these maximums be a parameter in the dataset for the model to learn the non-linear scaling required from the data itself. \n",
    "\n",
    "* Trying to use non-scaled data might be a good option if some way to initialize weights effectively is found, afterall, the data is expected to always have the same behaviour as it is supposed to represent real world physics which should be a stable dataset.\n",
    "\n",
    "* An \"energy\" term  that agglomerates dsplacements and forces cumulatively could also be possibly crafted to measure how much \"energy\" is being provided to the sample, and scaling that data using pi groups that way, although to avoid introducing non-data values, this energy term would just be a term with the dimensions of energy, instead of an actual energy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disp ** 2 * youngs_modulus / nodal_force\n",
      "disp * nodal_force / energy\n"
     ]
    }
   ],
   "source": [
    "pi_groups = ureg.pi_theorem({'nodal_force': '[force]',\n",
    "                        'disp': '[length]',\n",
    "                        'youngs_modulus':'[pressure]',\n",
    "                        'energy': '[energy]'})\n",
    "for group in pi_groups:\n",
    "      print(formatter(group.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a small dataset such as the one we have for the proof of concept, it would likely be useful to use some form of scaling which is more information dense such as introducing some engineering constant from outside of the data(yield stress, youngs modulus, etc). However, we don't really care for the performance of the proof of concept model, as long as it is able to learn some correct behaviour, so we're going to act as if I did have a dataset that can be considered \"exhaustive\" and simply scale it by the largest value of the dataset in that variable. This 'naive' approach involves inspecting the dataset to find that largest value, and is the one that likely scales best with a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create generator function to iterate through all samples\n",
    "def sample_iterator(samples_folder_path):\n",
    "    for i in "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
