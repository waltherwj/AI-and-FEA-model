{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Ansys Simulations\\Project\\2D\\v13_test\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "This notebook aims to, given the data exported from ANSYS, create new files that can be used in a convolutional neural network. The current idea on how to handle this is as follows:\n",
    "\n",
    "#### Concatenating\n",
    "The input data needs to be concatenated in one file with all nodes\n",
    "\n",
    "#### Data Scaling\n",
    "The input and output data have to be somehow scaled to ensure that the program can generalize to more scenarios. In an usual convolutional neural network, which usually deals with pixel-wise image data, this is simple, as all the values to be scaled are in the range from 0 to 255. This is not the case for this problem as the scale can vary arbitrarily from infinitesimals to very large numbers. What can be done, perhaps, is to create Pi groups to nondimensionalize the data, and then decide on a relevant non dimensional value to divide the data by to scale it. Another approach would be to use a relevant engineering value, such as the yield strength in our scenario, to scale it, however that requires previous engineering knowledge that we do not want to assume we have.\n",
    "\n",
    "#### Shaping\n",
    "After the scaling, this data has to be put in a format that can be used for a convolutional neural network. The kernel might have to be modified to travel between nodes instead of a grid, or maybe the data itself has to be transformed to be a continuous field which then could be split in a grid and a more usual kernel used to handle it. Yet a third option is to simply  leave the data as is and split it in a grid anyway. This last approach would require the grid to be such that it catches all nodes, so the smallest element size controls the grid size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
